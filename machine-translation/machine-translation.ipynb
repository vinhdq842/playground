{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install torch\n",
    "%pip install nltk\n",
    "%pip install wandb\n",
    "%pip install \"ipywidgets>=7.0,<8.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn.functional import pad\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(random_seed=42):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mt_eng_vietnamese (/home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d68e8e67ad945cda3db1b9820e3c21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 133318\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1269\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_data = load_dataset(\"mt_eng_vietnamese\",\"iwslt2015-en-vi\");mt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Bacteria are incredibly multi-drug-resistant right now , and that &apos;s because all of the antibiotics that we use kill bacteria .',\n",
       "  'vi': 'Vi khuẩn bây giờ có khả năng đề kháng rất nhiều loài thuốc và đó là vì tất cả các loại kháng sinh chúng ta sử dụng giết chết vi khuẩn .'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_data['train'][random.randint(0,len(mt_data['train']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(batch):\n",
    "    en = batch['translation']['en']\n",
    "    vi = batch['translation']['vi']\n",
    "    \n",
    "    en = re.sub(r'&.*;','',en)\n",
    "    en = re.sub(r'([0-9,\\.]{2,})(?![a-z\\-])','',en)\n",
    "    en = re.sub(r'[-\\.]{2,}[0-9\\.]*','',en)\n",
    "    en = ' '.join(list(filter(lambda x: len(x),en.split())))\n",
    "    batch['translation']['en'] = en\n",
    "    \n",
    "    vi = re.sub(r'&.*;','',vi)\n",
    "    vi = re.sub(r'[-\\.]{2,}[0-9\\.]*','',vi)\n",
    "    vi = re.sub(r'([0-9,\\.]{2,})(?![a-z\\-])','',vi)\n",
    "    vi = ' '.join(list(filter(lambda x: len(x),vi.split())))\n",
    "    batch['translation']['vi'] = vi\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-0ec0d62847218b8a.arrow\n",
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-a0b2f1e553bc8cc1.arrow\n",
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-a52147ac6173bebf.arrow\n"
     ]
    }
   ],
   "source": [
    "mt_data = mt_data.map(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-f499bd2143ab12bd.arrow\n",
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-a7cc12d5d234db64.arrow\n",
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-3a4a2326351661ae.arrow\n"
     ]
    }
   ],
   "source": [
    "mt_data = mt_data.filter(lambda x: len(x['translation']['en'].strip()) and len(x['translation']['vi'].strip()) and len(x['translation']['en'].strip().split()) < 30 and len(x['translation']['vi'].strip().split()) < 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "SOS = '<sos>'\n",
    "EOS = '<eos>'\n",
    "UNK = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(batch):\n",
    "    all_word_en = {k:v for k,v in Counter(' '.join([b['en'].lower() for b in batch['translation']]).split()).items() if v > 3}\n",
    "    all_word_vi = {k:v for k,v in Counter(' '.join([b['vi'].lower() for b in batch['translation']]).split()).items() if v > 3}\n",
    "    \n",
    "    return {'vocab_en':[all_word_en],'vocab_vi':[all_word_vi]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-50f13a4819ebb190.arrow\n",
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-9bcfd1c796158201.arrow\n",
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-251682ed2a91767c.arrow\n"
     ]
    }
   ],
   "source": [
    "vocabs = mt_data.map(get_vocab,batched=True,batch_size=-1,remove_columns=mt_data.column_names['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['vocab_en', 'vocab_vi'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['vocab_en', 'vocab_vi'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['vocab_en', 'vocab_vi'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_en = list(set(vocabs['train']['vocab_en'][0])|set(vocabs['test']['vocab_en'][0]) |set(vocabs['validation']['vocab_en'][0]))\n",
    "vocab_en = {v:k for k,v in enumerate([PAD,SOS,EOS,UNK] + sorted(vocab_en))}\n",
    "rev_vocab_en = {v:k for k,v in vocab_en.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_vi = list(set(vocabs['train']['vocab_vi'][0])|set(vocabs['test']['vocab_vi'][0]) |set(vocabs['validation']['vocab_vi'][0]))\n",
    "vocab_vi = {v:k for k,v in enumerate([PAD,SOS,EOS,UNK] + sorted(vocab_vi))}\n",
    "rev_vocab_vi = {v:k for k,v in vocab_vi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4964\n",
      "11306\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_vi))\n",
    "print(len(vocab_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab,emb_size,hid_size,dropout=0.5,pad=PAD):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.emb = nn.Embedding(len(vocab),emb_size,padding_idx=vocab[pad])\n",
    "        self.gru = nn.GRU(input_size=emb_size,hidden_size=hid_size,bidirectional=True)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hid_size*2,hid_size)\n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.emb(x))\n",
    "        outputs,hidden = self.gru(x)\n",
    "        hidden = torch.cat((hidden[-2],hidden[-1]),dim=-1)\n",
    "        \n",
    "        return outputs, torch.tanh(self.fc(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,hid_size):\n",
    "        super(Attention,self).__init__()\n",
    "        self.attn = nn.Linear(hid_size*3,hid_size)\n",
    "        self.v    = nn.Linear(hid_size,1,bias=False)\n",
    "    \n",
    "    def forward(self,hidden,encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        hidden  = hidden.unsqueeze(1).expand(-1,src_len,-1)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden,encoder_outputs.permute(1,0,2)),dim=-1)))\n",
    "        \n",
    "        attn = self.v(energy).squeeze(-1)\n",
    "        \n",
    "        return torch.softmax(attn,dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocab,emb_size,hid_size,attention,dropout=0.5,pad=PAD):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.emb = nn.Embedding(len(vocab),emb_size,padding_idx=vocab[pad])\n",
    "        self.gru = nn.GRU(input_size=hid_size*2 + emb_size,hidden_size=hid_size)\n",
    "        self.fc = nn.Linear(hid_size*3 + emb_size, len(vocab))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = attention\n",
    "        \n",
    "    def forward(self,x,hidden,encoder_outputs):\n",
    "        x = x.unsqueeze(0)\n",
    "        emb = self.dropout(self.emb(x))\n",
    "        attn = self.attention(hidden,encoder_outputs).unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        \n",
    "        weighted = torch.bmm(attn,encoder_outputs).permute(1,0,2)\n",
    "        gru_input = torch.cat((emb,weighted),dim=2)\n",
    "        output, hidden = self.gru(gru_input,hidden.unsqueeze(0))\n",
    "        assert (output == hidden).all(),(output,hidden)\n",
    "        \n",
    "        emb = emb.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        logits = self.fc(torch.cat((output,weighted,emb),dim=1))\n",
    "        \n",
    "        return logits,hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTModel(nn.Module):\n",
    "    def __init__(self,in_vocab,out_vocab,emb_size=300,hid_size=256,dropout=0.5,sos=SOS,eos=EOS,pad=PAD):\n",
    "        super(MTModel,self).__init__()\n",
    "        self.in_vocab = in_vocab\n",
    "        self.out_vocab  = out_vocab\n",
    "        self.rev_out_vocab = {v:k for k,v in out_vocab.items()}\n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "        self.pad = pad\n",
    "        self.encoder = Encoder(in_vocab,emb_size,hid_size,dropout,pad)\n",
    "        self.decoder = Decoder(out_vocab,emb_size,hid_size,Attention(hid_size),dropout,pad)\n",
    "        \n",
    "    def forward(self,src,trg,teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = len(self.out_vocab)\n",
    "        \n",
    "        outputs = torch.zeros(trg_len,batch_size,trg_vocab_size).to(src.device)\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1,trg_len):\n",
    "            logits, hidden = self.decoder(input,hidden,encoder_outputs)\n",
    "            outputs[t] = logits\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            input = trg[t,:] if teacher_force else logits.argmax(-1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def infer(self,src):\n",
    "        if len(src.shape) == 1:\n",
    "            src = src.unsqueeze(1)\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        input = (torch.LongTensor(batch_size * [self.out_vocab[self.sos]])).to(src.device)\n",
    "        max_len = 50\n",
    "        cur_len = 1\n",
    "        outputs = [input]\n",
    "        while not (outputs[-1] == self.out_vocab[self.eos]).all() and cur_len < max_len:\n",
    "            logits,hidden = self.decoder(input,hidden,encoder_outputs)\n",
    "            \n",
    "            outputs.append(logits.argmax(-1))\n",
    "            input = outputs[-1]\n",
    "            cur_len += 1\n",
    "        \n",
    "        res = [[] for _ in range(batch_size)]\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(batch_size):\n",
    "                res[j].append(self.rev_out_vocab[outputs[i][j].item()])\n",
    "        return res\n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self,src,trg,in_pad,out_pad):\n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.in_pad = in_pad\n",
    "        self.out_pad = out_pad\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index],self.trg[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def pad(self,inputs,PAD):\n",
    "        def pad_data(x, length, PAD):\n",
    "            x_padded = pad(\n",
    "                x, (0, length - x.shape[0]), mode=\"constant\", value=PAD\n",
    "            )\n",
    "            return x_padded\n",
    "\n",
    "        max_len = max((len(x) for x in inputs))\n",
    "        padded = torch.stack([pad_data(torch.LongTensor(x), max_len, PAD) for x in inputs])\n",
    "\n",
    "        return padded\n",
    "\n",
    "    \n",
    "    def collate_fn(self,batch):\n",
    "        src = []\n",
    "        trg = []\n",
    "        \n",
    "        for s,t in batch:\n",
    "            src.append(s)\n",
    "            trg.append(t)\n",
    "        return self.pad(src,self.in_pad), self.pad(trg,self.out_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2id(batch):\n",
    "    batch['input_ids_vi'] = [[vocab_vi[SOS]] + [vocab_vi.get(word,vocab_vi[UNK]) for word in b['vi'].lower().split()] + [vocab_vi[EOS]] for b in batch['translation']]    \n",
    "    batch['input_ids_en'] = [[vocab_en[SOS]] + [vocab_en.get(word,vocab_en[UNK]) for word in b['en'].lower().split()] + [vocab_en[EOS]] for b in batch['translation']]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-9f6d24d5cac99944.arrow\n",
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-ca7977cf65596fa3.arrow\n",
      "Loading cached processed dataset at /home/aimenext/.cache/huggingface/datasets/mt_eng_vietnamese/iwslt2015-en-vi/1.0.0/53add551a01e9874588066f89d42925f9fad43db347199dad00f7e4b0c905a71/cache-b96d1bb261464c8e.arrow\n"
     ]
    }
   ],
   "source": [
    "mt_ids = mt_data.map(text2id,batch_size=8,batched=True,remove_columns=mt_data.column_names['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> những cô gái này đã rất may mắn . <eos>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([rev_vocab_vi[i] for i in mt_ids['validation']['input_ids_vi'][25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47768754038252614\n",
      "0.4776875403825262\n"
     ]
    }
   ],
   "source": [
    "def bleu(candidate,reference):\n",
    "    overlap = 0\n",
    "    can_counter = Counter(candidate)\n",
    "    ref_counter = Counter(reference)\n",
    "    for can in candidate:\n",
    "        overlap += min(can_counter.get(can),ref_counter.get(can,0))\n",
    "    \n",
    "    return min(1,np.exp(1-len(reference)/len(candidate))) * overlap / sum(can_counter.values())\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "references = [['my', 'lol', 'correct', 'sentence']]\n",
    "candidates = ['my', 'phucking','sentence']\n",
    "print(sentence_bleu(references, candidates,weights=(1,)))\n",
    "print(bleu(candidates,references[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TextDataset(mt_ids['test']['input_ids_en'],mt_ids['test']['input_ids_vi'],vocab_en['<pad>'],vocab_vi['<pad>'])\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=16,collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(mt_ids['train']['input_ids_en'],mt_ids['train']['input_ids_vi'],vocab_en['<pad>'],vocab_vi['<pad>'])\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=8,collate_fn=train_dataset.collate_fn,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/disk1/vinhdq/mt-playground/wandb/run-20230103_133313-3f9qkmzj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/quangvinh0842/my-simple-nmt/runs/3f9qkmzj\" target=\"_blank\">avid-sun-9</a></strong> to <a href=\"https://wandb.ai/quangvinh0842/my-simple-nmt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/quangvinh0842/my-simple-nmt/runs/3f9qkmzj?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f83718dc8b0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='my-simple-nmt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device,model,num_steps,train_dataloader,val_dataloader,optimizer,criterion,metric_fn,step=0,checkpoint_step=5000):\n",
    "    outer_bar = tqdm(total=num_steps,desc='Training', position=0)\n",
    "    outer_bar.n = step\n",
    "    outer_bar.last_print_n = step \n",
    "    outer_bar.refresh()\n",
    "    epoch = 1\n",
    "    while True:\n",
    "        inner_bar = tqdm(total=len(train_dataloader),desc=f'Epoch {epoch}',position=1)\n",
    "        model.train()\n",
    "        \n",
    "        for X, y in train_dataloader:\n",
    "            X,y = X.to(device).permute(1,0),y.to(device).permute(1,0)\n",
    "            outputs = model(X,y)\n",
    "            \n",
    "            loss = criterion(outputs[1:].contiguous().view(-1,outputs.shape[-1]),y[1:].contiguous().view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            step += 1\n",
    "            outer_bar.update(1)\n",
    "            inner_bar.update(1)\n",
    "            wandb.log({'trainning loss':loss.item()})\n",
    "                        \n",
    "            if step % checkpoint_step == 0:\n",
    "                outer_bar.write(f'Training loss at step {step}: {loss.item():.6}')\n",
    "                torch.save({\"model\":model.state_dict(),\"optimizer\":optimizer.state_dict(),\"step\":step},f'checkpoints/checkpoint-{step}.pth')            \n",
    "                model.eval()\n",
    "                val_loss = []\n",
    "                with torch.no_grad():\n",
    "                    for X, y in val_dataloader:\n",
    "                        X,y = X.to(device).permute(1,0),y.to(device).permute(1,0)\n",
    "                        outputs = model(X,y)\n",
    "\n",
    "                        val_loss.append(criterion(outputs[1:].contiguous().view(-1,outputs.shape[-1]),y[1:].contiguous().view(-1)).item())\n",
    "                \n",
    "                model.train()\n",
    "                outer_bar.write(f'Val loss at step {step}: {np.mean(val_loss):.6}')\n",
    "                wandb.log({'val loss':np.mean(val_loss)})\n",
    "            if step >= num_steps:\n",
    "                return\n",
    "                \n",
    "        epoch += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 12,194,812 trainable params\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = MTModel(vocab_en,vocab_vi).to(device)\n",
    "print(f'The model has {count_params(model):,} trainable params')\n",
    "optimizer = torch.optim.Adam(model.parameters(),0.0009)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_vi[PAD])\n",
    "num_steps = 200000\n",
    "metric_fn = lambda y_hats,ys: [bleu(y_hat,y) for y_hat,y in zip(y_hats,ys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa55d19c3dd245a9994fd0a1ad6c22d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d763146d5c40bcb0a386b4009a941b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/12030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 5000: 4.67544\n",
      "Val loss at step 5000: 4.4716\n",
      "Training loss at step 10000: 4.29089\n",
      "Val loss at step 10000: 4.29298\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cbd56103a24b50aeb2f8bc84a9a882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/12030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 15000: 4.74001\n",
      "Val loss at step 15000: 4.25563\n",
      "Training loss at step 20000: 4.7032\n",
      "Val loss at step 20000: 4.11833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e151fc43dd2b4c7bbc5a1e83f6b24248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/12030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 25000: 4.27266\n",
      "Val loss at step 25000: 4.12298\n",
      "Training loss at step 30000: 4.25208\n",
      "Val loss at step 30000: 4.08961\n",
      "Training loss at step 35000: 4.24699\n",
      "Val loss at step 35000: 4.08566\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410ca94f80f94744a511531ccbd2c278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/12030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 40000: 3.82618\n",
      "Val loss at step 40000: 4.1183\n",
      "Training loss at step 45000: 4.2499\n",
      "Val loss at step 45000: 4.00975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf82f854b894cd6b6ed74292859637b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/12030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 50000: 4.71677\n",
      "Val loss at step 50000: 4.08016\n",
      "Training loss at step 55000: 3.66346\n",
      "Val loss at step 55000: 4.14163\n",
      "Training loss at step 60000: 5.24408\n",
      "Val loss at step 60000: 4.14183\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c5031b09e7438db3c47c1fc2402ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/12030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 65000: 3.09483\n",
      "Val loss at step 65000: 4.04568\n",
      "Training loss at step 70000: 3.98008\n",
      "Val loss at step 70000: 4.00273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(device,model,num_steps,train_dataloader,test_dataloader,optimizer,criterion,metric_fn)\n",
      "Cell \u001b[0;32mIn[68], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(device, model, num_steps, train_dataloader, val_dataloader, optimizer, criterion, metric_fn, step, checkpoint_step)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[1;32m     12\u001b[0m     X,y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m),y\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     outputs \u001b[39m=\u001b[39m model(X,y)\n\u001b[1;32m     15\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs[\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,outputs\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]),y[\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/mnt/disk1/vinhdq/mt-playground/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 24\u001b[0m, in \u001b[0;36mMTModel.forward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m trg[\u001b[39m0\u001b[39m,:]\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,trg_len):\n\u001b[0;32m---> 24\u001b[0m     logits, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\u001b[39minput\u001b[39;49m,hidden,encoder_outputs)\n\u001b[1;32m     25\u001b[0m     outputs[t] \u001b[39m=\u001b[39m logits\n\u001b[1;32m     27\u001b[0m     teacher_force \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandom() \u001b[39m<\u001b[39m teacher_forcing_ratio\n",
      "File \u001b[0;32m/mnt/disk1/vinhdq/mt-playground/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     16\u001b[0m weighted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(attn,encoder_outputs)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m     17\u001b[0m gru_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((emb,weighted),dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru(gru_input,hidden\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[1;32m     19\u001b[0m \u001b[39massert\u001b[39;00m (output \u001b[39m==\u001b[39m hidden)\u001b[39m.\u001b[39mall(),(output,hidden)\n\u001b[1;32m     21\u001b[0m emb \u001b[39m=\u001b[39m emb\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/mnt/disk1/vinhdq/mt-playground/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/disk1/vinhdq/mt-playground/.env/lib/python3.10/site-packages/torch/nn/modules/rnn.py:955\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    954\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    956\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    957\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    959\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(device,model,num_steps,train_dataloader,test_dataloader,optimizer,criterion,metric_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a24e62f64e04b85ba1851b76d8a7849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>trainning loss</td><td>▇██▇▅▃▄▄▄▂▃▅▃▅▃▂▂▇▂▄▅▆▅▄▃▃▄▃▃▅▁▂▁▃▃▆▄▁▃▂</td></tr><tr><td>val loss</td><td>█▅▅▃▃▂▂▃▁▂▃▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>trainning loss</td><td>3.95042</td></tr><tr><td>val loss</td><td>4.00273</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">avid-sun-9</strong>: <a href=\"https://wandb.ai/quangvinh0842/my-simple-nmt/runs/3f9qkmzj\" target=\"_blank\">https://wandb.ai/quangvinh0842/my-simple-nmt/runs/3f9qkmzj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230103_133313-3f9qkmzj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 1865, 4182,  795, 2865,   10, 4182, 2644, 3422,    3, 3949, 2163,\n",
      "         4824, 2961, 4261, 4036, 3901, 3901, 1322, 4378, 4182, 3865, 1597,  303,\n",
      "            2],\n",
      "        [   0, 4182, 4770, 3413, 4287, 1591, 4458, 4824, 2961, 4182,   12,    2,\n",
      "         4182,   12,    2,   12,    2,   12,   12,   12,   12,   12,    2, 4182,\n",
      "           12]], device='cuda:1')\n",
      "tensor([[   1, 1865, 4182,  795, 2865,   10, 4182, 2644, 3422,    3, 3949, 2163,\n",
      "         4824, 2961, 4261, 2843, 4036, 3901, 1322, 4378, 4182, 3865, 1597,  303,\n",
      "            2],\n",
      "        [   1, 4182, 4770, 3413, 4287, 1591, 4458, 4824, 2961, 4182,   12,    2,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0]])\n",
      "tensor([[   0, 1261, 4777,  894, 4182, 1931, 2640,   10, 4378,  372, 3830, 4182,\n",
      "         3840,  623, 4285, 3167,  664, 4784,   12,    2,    2,   12,   12,    2,\n",
      "            2,    2,    2,   12],\n",
      "        [   0, 2827, 4383, 2575, 2680,  894, 2953,   10, 2544, 4182, 2329, 4458,\n",
      "         2785, 2575, 2168, 3860, 4284, 2575, 2704,  663, 1129,  805,  678, 2164,\n",
      "         4475, 2544,   12,    2]], device='cuda:1')\n",
      "tensor([[   1, 1261, 4777,  894, 4182, 1931, 2640,   10, 4378,  372, 3830, 4182,\n",
      "         3840,  623, 4285, 3167,  664, 4784,   12,    2,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   1, 2827, 4383, 2575, 2680,  894, 2953,   10, 2544, 4182, 2329, 4458,\n",
      "         2785, 2575, 2168, 3860, 4284, 2575, 2704,  663, 1129,  805,  678, 2164,\n",
      "         4475, 2544,   12,    2]])\n"
     ]
    }
   ],
   "source": [
    "for i,(X,y) in enumerate(test_dataloader):\n",
    "    print(model(X.to(device).permute(1,0),y.to(device).permute(1,0)).argmax(-1).permute(1,0))\n",
    "    print(y)\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/checkpoint-200000.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "step = checkpoint['step']\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "@interact\n",
    "def translate(text=\"You are so bad\"):\n",
    "    input_ids = [vocab_en[SOS]] +  [vocab_en.get(word,vocab_en[UNK]) for word in text.lower().split()]+ [vocab_en[EOS]]\n",
    "    display(' '.join(model.infer(torch.LongTensor(input_ids).to(device))[0][1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f7bce8ee8dacad507944f5e423faca4a7e29267a137e5ad77be2e0dfbbbd28a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
